{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_H = 262\n",
    "IMAGE_W = 262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor()   # 转换成Tensor形式，并且数值归一化到[0.0, 1.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogsVSCatsDataset(data.Dataset):     \n",
    "    def __init__(self, mode, dir):          \n",
    "        self.mode = mode\n",
    "        self.list_img = []                  \n",
    "        self.list_label = []               \n",
    "        self.data_size = 0                  \n",
    "        self.transform = data_transform     \n",
    "\n",
    "        if self.mode == 'train':            \n",
    "            dir = dir + '/train/'           \n",
    "            for file in os.listdir(dir):    \n",
    "                self.list_img.append(dir + file)       \n",
    "                self.data_size += 1                     \n",
    "                name = file.split(sep='.')              \n",
    "               \n",
    "                if name[0] == 'cat':\n",
    "                    self.list_label.append(0)        \n",
    "                else:\n",
    "                    self.list_label.append(1)  \n",
    "                    \n",
    "        elif self.mode == 'test':           \n",
    "            dir = dir + '/test/'         \n",
    "            for file in os.listdir(dir):\n",
    "                self.list_img.append(dir + file)    \n",
    "                self.data_size += 1\n",
    "                self.list_label.append(2)   \n",
    "                \n",
    "        elif self.mode == 'valid':            \n",
    "            dir = dir + '/valid/'           \n",
    "            for file in os.listdir(dir):    \n",
    "                self.list_img.append(dir + file)       \n",
    "                self.data_size += 1                     \n",
    "                name = file.split(sep='.')              \n",
    "                \n",
    "                if name[0] == 'cat':\n",
    "                    self.list_label.append(0)         \n",
    "                else:\n",
    "                    self.list_label.append(1)         \n",
    "        else:\n",
    "            return print('Undefined Dataset!')\n",
    "\n",
    "    def __getitem__(self, item):            \n",
    "        if self.mode == 'train':                                        \n",
    "            img = Image.open(self.list_img[item])                       \n",
    "            img = img.resize((IMAGE_H, IMAGE_W))                        \n",
    "            img = np.array(img)[:, :, :3]                               \n",
    "            label = self.list_label[item]                               \n",
    "            return self.transform(img), torch.LongTensor([label])       \n",
    "        elif self.mode == 'test':                                      \n",
    "            img = Image.open(self.list_img[item])\n",
    "            img = img.resize((IMAGE_H, IMAGE_W))\n",
    "            img = np.array(img)[:, :, :3]\n",
    "            return self.transform(img)                                  \n",
    "        if self.mode == 'valid':                                        \n",
    "            img = Image.open(self.list_img[item])                       \n",
    "            img = img.resize((IMAGE_H, IMAGE_W))                        \n",
    "            img = np.array(img)[:, :, :3]                             \n",
    "            label = self.list_label[item]                              \n",
    "            return self.transform(img), torch.LongTensor([label])       \n",
    "        else:\n",
    "            print('None')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size               # 返回数据集大小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DVCD = DogsVSCatsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):                                       \n",
    "    def __init__(self):                                     \n",
    "        super(Net, self).__init__()                         \n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, 3)   \n",
    "        self.conv2 = torch.nn.Conv2d(16, 16, 3)  \n",
    "\n",
    "        self.fc1 = nn.Linear(64*64*16, 64*8)                \n",
    "        self.fc2 = nn.Linear(64*8, 64*2)                       \n",
    "        self.fc3 = nn.Linear(64*2,64)                         \n",
    "        self.fc4 = nn.Linear(64,2)  \n",
    "    def forward(self, x):   \n",
    "       \n",
    "        x = self.conv1(x)                   \n",
    "        x = F.relu(x)                      \n",
    "        x = F.max_pool2d(x, 2)   \n",
    "        \n",
    "        x = self.conv2(x)                   \n",
    "        x = F.relu(x)                       \n",
    "        x = F.max_pool2d(x, 2)              \n",
    "        \n",
    "        x = x.view(x.size()[0], -1)        \n",
    "        x = F.relu(self.fc1(x))             \n",
    "        x = F.relu(self.fc2(x))             \n",
    "        x = F.relu(self.fc3(x)) \n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return F.softmax(x, dim=1)  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'C:/ml_data/catsvsdogs/'             # 数据集路径\n",
    "model_cp = 'C:/ml_data/catsvsdogs/'               # 网络参数保存位置\n",
    "batch_size = 32                     # batch_size大小\n",
    "lr = 0.0001  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里是训练完，进行预测的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn_epoch_11 = torch.load('C:/ml_data/catsvsdogs/epoch_0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(model):  \n",
    "    with torch.no_grad():\n",
    "        result = []\n",
    "        datafile = DVCD('test', dataset_dir)  \n",
    "        dataloader = DataLoader(datafile, batch_size=batch_size, shuffle=False)     \n",
    "        print('Dataset loaded! length of test set is {0}'.format(len(datafile)))\n",
    "        for img in dataloader:\n",
    "\n",
    "            img= Variable(img).cuda()\n",
    "            outputs = model(img).to('cpu')\n",
    "            result += outputs\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded! length of test set is 12500\n"
     ]
    }
   ],
   "source": [
    "result = pred(model_cnn_epoch_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = []\n",
    "for item in result:\n",
    "    predict_result.append(torch.argmax(item, dim=0).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测结果： predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下面是训练的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = Net().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(model_cnn,'C:/ml_data/catsvsdogs/model_init.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = torch.load('C:/ml_data/catsvsdogs/model_init.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr=lr) \n",
    "criterion = torch.nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, folder_name, criterion):                                      \n",
    "\n",
    "    datafile = DVCD(folder_name, dataset_dir)  \n",
    "    dataloader = DataLoader(datafile, batch_size=batch_size, shuffle=True)     \n",
    "    print('Dataset loaded! length of test set is {0}'.format(len(datafile)))\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    for img,label in dataloader:\n",
    "        cnt += 1\n",
    "        total += batch_size\n",
    "        img= Variable(img).cuda()\n",
    "        label = Variable(label).cuda() \n",
    "        \n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, label.squeeze())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        pred_y = []\n",
    "        for item in outputs:\n",
    "            if item[0].item() > item[1].item():\n",
    "                pred_y.append(0)\n",
    "            else:\n",
    "                pred_y.append(1)\n",
    "        \n",
    "        \n",
    "        for i in range(len(label)):\n",
    "            if pred_y[i] == label[i]:\n",
    "                correct += 1\n",
    "    return correct, total, total_loss/cnt\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch_acc = []\n",
    "train_epoch_loss = []\n",
    "valid_epoch_acc = []\n",
    "valid_epoch_loss = []\n",
    "\n",
    "train_iter_loss = []\n",
    "epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, epochs):\n",
    "    global epoch_count\n",
    "    epoch_count += 1\n",
    "    datafile = DVCD('train', dataset_dir)                                                      \n",
    "    dataloader = DataLoader(datafile, batch_size=batch_size, shuffle=True)     \n",
    "    print('Dataset loaded! length of train set is {0}'.format(len(datafile)))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        cnt = 0  \n",
    "        for img, label in dataloader:                                          \n",
    "            img, label = Variable(img).cuda(), Variable(label).cuda()           \n",
    "            out = model(img) \n",
    "            \n",
    "            loss = criterion(out, label.squeeze())      \n",
    "            loss.backward()                             \n",
    "            optimizer.step()                           \n",
    "            optimizer.zero_grad()                      \n",
    "            cnt += 1\n",
    "            train_iter_loss.append(loss.item())\n",
    "            if cnt % 100 == 0:\n",
    "                print('Epoch: {0}/{1}, iter {2}/{3}, train_loss {4}'.format(epoch+1, epochs, cnt+1, int(20000/32)+1,loss/batch_size))   \n",
    "            \n",
    "        correct_count,total_count, valid_loss = test(model,'valid', criterion)\n",
    "        valid_epoch_acc.append(correct_count/total_count*100)\n",
    "        valid_epoch_loss.append(valid_loss)\n",
    "        \n",
    "        correct_count,total_count, train_loss = test(model,'train', criterion)\n",
    "        train_epoch_acc.append(correct_count/total_count*100)\n",
    "        train_epoch_loss.append(train_loss)\n",
    "        torch.save(model,'C:/ml_data/catsvsdogs/epoch_{}.pth'.format(epoch_count))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded! length of train set is 20000\n",
      "Epoch: 1/11, iter 101/626, train_loss 0.021041983738541603\n",
      "Epoch: 1/11, iter 201/626, train_loss 0.02064378932118416\n",
      "Epoch: 1/11, iter 301/626, train_loss 0.020455822348594666\n",
      "Epoch: 1/11, iter 401/626, train_loss 0.02094901166856289\n",
      "Epoch: 1/11, iter 501/626, train_loss 0.021509183570742607\n",
      "Epoch: 1/11, iter 601/626, train_loss 0.018877752125263214\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 2/11, iter 101/626, train_loss 0.021012594923377037\n",
      "Epoch: 2/11, iter 201/626, train_loss 0.019397538155317307\n",
      "Epoch: 2/11, iter 301/626, train_loss 0.02126147598028183\n",
      "Epoch: 2/11, iter 401/626, train_loss 0.01920810341835022\n",
      "Epoch: 2/11, iter 501/626, train_loss 0.020632024854421616\n",
      "Epoch: 2/11, iter 601/626, train_loss 0.020515073090791702\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 3/11, iter 101/626, train_loss 0.017728760838508606\n",
      "Epoch: 3/11, iter 201/626, train_loss 0.01928393542766571\n",
      "Epoch: 3/11, iter 301/626, train_loss 0.017732104286551476\n",
      "Epoch: 3/11, iter 401/626, train_loss 0.014783398248255253\n",
      "Epoch: 3/11, iter 501/626, train_loss 0.016493869945406914\n",
      "Epoch: 3/11, iter 601/626, train_loss 0.016402732580900192\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 4/11, iter 101/626, train_loss 0.01771424524486065\n",
      "Epoch: 4/11, iter 201/626, train_loss 0.01651111990213394\n",
      "Epoch: 4/11, iter 301/626, train_loss 0.016380490735173225\n",
      "Epoch: 4/11, iter 401/626, train_loss 0.014363948255777359\n",
      "Epoch: 4/11, iter 501/626, train_loss 0.015486259944736958\n",
      "Epoch: 4/11, iter 601/626, train_loss 0.01539276260882616\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 5/11, iter 101/626, train_loss 0.017313063144683838\n",
      "Epoch: 5/11, iter 201/626, train_loss 0.015428677201271057\n",
      "Epoch: 5/11, iter 301/626, train_loss 0.0156768336892128\n",
      "Epoch: 5/11, iter 401/626, train_loss 0.015087990090250969\n",
      "Epoch: 5/11, iter 501/626, train_loss 0.016137974336743355\n",
      "Epoch: 5/11, iter 601/626, train_loss 0.017600463703274727\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 6/11, iter 101/626, train_loss 0.015407271683216095\n",
      "Epoch: 6/11, iter 201/626, train_loss 0.01698306016623974\n",
      "Epoch: 6/11, iter 301/626, train_loss 0.014119617640972137\n",
      "Epoch: 6/11, iter 401/626, train_loss 0.01558072678744793\n",
      "Epoch: 6/11, iter 501/626, train_loss 0.017107758671045303\n",
      "Epoch: 6/11, iter 601/626, train_loss 0.015127689577639103\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 7/11, iter 101/626, train_loss 0.014903362840414047\n",
      "Epoch: 7/11, iter 201/626, train_loss 0.015136728063225746\n",
      "Epoch: 7/11, iter 301/626, train_loss 0.01616215705871582\n",
      "Epoch: 7/11, iter 401/626, train_loss 0.014061729423701763\n",
      "Epoch: 7/11, iter 501/626, train_loss 0.016540953889489174\n",
      "Epoch: 7/11, iter 601/626, train_loss 0.01275444682687521\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 8/11, iter 101/626, train_loss 0.0136500159278512\n",
      "Epoch: 8/11, iter 201/626, train_loss 0.012190195731818676\n",
      "Epoch: 8/11, iter 301/626, train_loss 0.01359669677913189\n",
      "Epoch: 8/11, iter 401/626, train_loss 0.013910013251006603\n",
      "Epoch: 8/11, iter 501/626, train_loss 0.01572878658771515\n",
      "Epoch: 8/11, iter 601/626, train_loss 0.015190023928880692\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 9/11, iter 101/626, train_loss 0.015027463436126709\n",
      "Epoch: 9/11, iter 201/626, train_loss 0.015073639340698719\n",
      "Epoch: 9/11, iter 301/626, train_loss 0.01296953670680523\n",
      "Epoch: 9/11, iter 401/626, train_loss 0.012391633354127407\n",
      "Epoch: 9/11, iter 501/626, train_loss 0.01275894045829773\n",
      "Epoch: 9/11, iter 601/626, train_loss 0.011498484760522842\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 10/11, iter 101/626, train_loss 0.012299960479140282\n",
      "Epoch: 10/11, iter 201/626, train_loss 0.012147662229835987\n",
      "Epoch: 10/11, iter 301/626, train_loss 0.011279218830168247\n",
      "Epoch: 10/11, iter 401/626, train_loss 0.010490639135241508\n",
      "Epoch: 10/11, iter 501/626, train_loss 0.012001365423202515\n",
      "Epoch: 10/11, iter 601/626, train_loss 0.012823724187910557\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n",
      "Epoch: 11/11, iter 101/626, train_loss 0.011834569275379181\n",
      "Epoch: 11/11, iter 201/626, train_loss 0.011417079716920853\n",
      "Epoch: 11/11, iter 301/626, train_loss 0.014170217327773571\n",
      "Epoch: 11/11, iter 401/626, train_loss 0.011087481863796711\n",
      "Epoch: 11/11, iter 501/626, train_loss 0.012762804515659809\n",
      "Epoch: 11/11, iter 601/626, train_loss 0.01306126732379198\n",
      "Dataset loaded! length of test set is 5000\n",
      "Dataset loaded! length of test set is 20000\n"
     ]
    }
   ],
   "source": [
    "train(model_cnn, optimizer, criterion,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch为11的时候，验证集acc最高,loss最小，所以设置epoch为11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
